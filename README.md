# AI-Rank
AI-Rank是面向中国AI产业界的，对AI系统进行综合评价的评估体系。

随着AI技术的发展，各类AI计算设备（如：云服务器、显卡、芯片等）、各类AI框架如（Paddle、TensorFlow、PyTorch）等应运而生。用户在制定符合自身的深度学习应用方案时，难免在软件、硬件选型上缺少对比数据。

AI-Rank本着公开、公正、公平的原则，制定尽可能全面的综合评估体系，并广纳各类AI产品性能数据。希望能够切实挖掘人工智能产业发展过程中的需求与痛点，探究AI未来的趋势，助力全产业AI相关企业的发展和AI技术的进步。

AI-Rank主要面向产业应用，为应用放提供决策所需参考数据。在制定评估体系上，更多的考虑普适性、易用性、落地能力等。因此，在指标选取上与业界已有的性能评估系统存在较大差别。AI-Rank提供的数据将更实用、更贴近应用。

- 多条赛车道
AI-Rank通过如下三个方向评价AI方案：
    - 硬件赛道：通过设立全面的测评指标，考察算法与硬件构建的系统的优异性能。不限制平台，不限制硬件。
    - 软件赛道：在同样的硬件设置上,综合考虑多种典型硬件配置，考察算法框架的综合表现, 例如模型覆盖率、性能表现等。
    - 特色赛道：芯片、操作系统、训练和推理框架统一要求国产，并设定适合国产化的指标，以测促产，以测促研。
- 多种环境
AI-Rank关注被测AI方案在不同运行环境下的表现：
    - 云端训练：评价被测AI方案，在云端环境下，进行模型训练的整体表现。
    - 云端推理：评价被测AI方案，在云端环境下，进行模型推理的整体表现。
    - 终端推理：评价被测AI方案，在终端设备上，进行模型推理的整体表现。

我们将不断更新迭代，逐步完善以上各种评估场景。

# 成果
目前AI-Rank已收到XXX个数据，请参见:
- [《AI-Rank Software Result》](./result/software_result.md)
- [《AI-Rank Hardware Result》](./result/hardware_result.md)

# 如何参与评估？
参与AI-Rank的评估，分为提交、审核、公布三个阶段。
## 提交
提交阶段，参与方首先需注册GitHub并创建专属repo。然后将所有评审资料提交到该GitHub repo中。之后，向AI-Rank官方邮箱（XXX@XXX.com）发送申请评估邮件。AI-Rank将在5个工作日内予以回复是否受理，并在回复后10个工作日内启动审核工作。
不同赛道所需提交的资料不同，具体参考如下链接：
- [硬件赛道](./hardware/README.md)
- [软件赛道](./software/README.md)
- [特色赛道](./nationalization/README.md)

## 审核
审核阶段分为3个子阶段：初审、答辩、确认
### 初审
AI-Rank将组织领域专家，对提交放提供的资料进行审核，对提供的代码/脚本进行复现，将复现结果与提交放提供日志、数据做比对。
经过初审，专家组将整理问题清单，反馈给提交方。

领域专家组由AI-Rank组建，当前成员参加[《专家组名单》](./expert.md)。

### 答辩
由提交方根据问题清单，准备答辩材料。由AI-Rank组织提交方与专家组进行现场讨论。现场，专家组也可再补充新的问题。问题清单所有问题，均需有明确结论。
答辩内容已纪要形式，伴随最终数据共同公布。

### 确认
答辩后，可能部分数据有所调整。也可能无任何调整。AI-Rank将出具确认数据的书面文件，提交方确认后方可发布。

## 公布
由AI-Rank，将确认后的数据，在本GitHub repo中整理、发布。

ML-Perf 提交参考 https://github.com/mlperf/training_policies/blob/master/training_rules.adoc